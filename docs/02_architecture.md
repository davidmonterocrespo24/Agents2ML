# Arquitectura del Sistema Multi-Agent AutoML

## ğŸ—ï¸ VisiÃ³n General de la Arquitectura

El Sistema Multi-Agent AutoML estÃ¡ construido con una **arquitectura modular y distribuida** que permite escalabilidad, mantenibilidad y extensibilidad. El sistema utiliza un enfoque de **microservicios con agentes especializados** que colaboran para completar el pipeline de Machine Learning.

## ğŸ¯ Principios de DiseÃ±o

### ğŸ”„ **SeparaciÃ³n de Responsabilidades**
- Cada agente tiene una funciÃ³n especÃ­fica y bien definida
- ComunicaciÃ³n entre agentes a travÃ©s de interfaces estÃ¡ndar
- Acoplamiento dÃ©bil entre componentes

### ğŸ›¡ï¸ **Seguridad por DiseÃ±o**
- EjecuciÃ³n aislada en contenedores Docker
- ValidaciÃ³n de entrada en todos los puntos
- Logs detallados para auditorÃ­a

### ğŸ“ˆ **Escalabilidad Horizontal**
- Arquitectura stateless permite mÃºltiples instancias
- Cola de trabajos para manejar carga
- Base de datos optimizada para concurrencia

## ğŸ›ï¸ Arquitectura de Alto Nivel

```mermaid
graph TB
    subgraph "Frontend"
        UI[Web Interface]
        API[REST API]
    end
    
    subgraph "Core System"
        PO[Pipeline Orchestrator]
        QM[Queue Manager]
        DB[(Database)]
    end
    
    subgraph "AI Agents"
        UPA[UserProxyAgent]
        DPA[DataProcessorAgent]
        MBA[ModelBuilderAgent]
        CEA[CodeExecutorAgent]
        AA[AnalystAgent]
        PA[PredictionAgent]
        VA[VisualizationAgent]
    end
    
    subgraph "External Services"
        LLM[gpt-oss:120b<br/>Ollama/HuggingFace]
        H2O[H2O AutoML]
        DOCKER[Docker Runtime]
    end
    
    UI --> API
    API --> PO
    PO --> QM
    QM --> DB
    
    PO --> UPA
    UPA --> DPA
    UPA --> MBA
    UPA --> CEA
    UPA --> AA
    UPA --> PA
    UPA --> VA
    
    DPA --> LLM
    MBA --> LLM
    CEA --> DOCKER
    CEA --> H2O
    PA --> LLM
    VA --> LLM
```

## ğŸ­ Los 7 Agentes Especializados

### ğŸ¤ **UserProxyAgent**
**Rol**: Coordinador general del sistema
- **Responsabilidades**:
  - OrquestaciÃ³n del flujo de trabajo
  - ComunicaciÃ³n entre agentes
  - GestiÃ³n de errores y reintentos
  - Reporte de progreso al usuario

### ğŸ“Š **DataProcessorAgent**
**Rol**: Especialista en anÃ¡lisis de datos
- **Responsabilidades**:
  - DetecciÃ³n automÃ¡tica de separadores y encoding
  - AnÃ¡lisis estadÃ­stico del dataset
  - IdentificaciÃ³n de tipos de datos
  - DetecciÃ³n de valores nulos y outliers
  - Sugerencia de columnas objetivo

### ğŸ§  **ModelBuilderAgent**
**Rol**: Arquitecto de modelos ML
- **Responsabilidades**:
  - GeneraciÃ³n de cÃ³digo Python para H2O AutoML
  - SelecciÃ³n de algoritmos apropiados
  - ConfiguraciÃ³n de hiperparÃ¡metros
  - OptimizaciÃ³n del pipeline de entrenamiento

### âš¡ **CodeExecutorAgent**
**Rol**: Ejecutor seguro de cÃ³digo
- **Responsabilidades**:
  - EjecuciÃ³n de cÃ³digo en contenedores Docker
  - InstalaciÃ³n automÃ¡tica de dependencias
  - Monitoreo de recursos y timeout
  - Captura de logs y errores

### ğŸ” **AnalystAgent**
**Rol**: Inspector de calidad
- **Responsabilidades**:
  - ValidaciÃ³n de cÃ³digo generado
  - EvaluaciÃ³n de resultados de modelos
  - AnÃ¡lisis de mÃ©tricas de rendimiento
  - GeneraciÃ³n de recomendaciones

### ğŸ¯ **PredictionAgent**
**Rol**: Generador de predicciones
- **Responsabilidades**:
  - Carga de modelos entrenados
  - GeneraciÃ³n de datos futuros
  - AplicaciÃ³n de modelos para predicciones
  - CÃ¡lculo de intervalos de confianza

### ğŸ“ˆ **VisualizationAgent**
**Rol**: Creador de visualizaciones
- **Responsabilidades**:
  - GeneraciÃ³n de grÃ¡ficos profesionales
  - CombinaciÃ³n de datos histÃ³ricos y predicciones
  - ExportaciÃ³n en mÃºltiples formatos
  - OptimizaciÃ³n para diferentes dispositivos

## ğŸ”„ Flujo de Datos del Sistema

### **Fase 1: Ingesta y AnÃ¡lisis**
```mermaid
sequenceDiagram
    participant U as Usuario
    participant API as REST API
    participant PO as Pipeline Orchestrator
    participant DPA as DataProcessorAgent
    participant DB as Database
    
    U->>API: Upload CSV + Objetivo
    API->>PO: Crear Pipeline
    PO->>DPA: Analizar Dataset
    DPA->>DPA: Detectar formato y estructura
    DPA->>DPA: AnÃ¡lisis estadÃ­stico
    DPA->>DB: Guardar anÃ¡lisis
    DPA->>PO: Reporte completado
    PO->>API: Status actualizado
    API->>U: AnÃ¡lisis completado
```

### **Fase 2: Entrenamiento de Modelos**
```mermaid
sequenceDiagram
    participant PO as Pipeline Orchestrator
    participant MBA as ModelBuilderAgent
    participant CEA as CodeExecutorAgent
    participant AA as AnalystAgent
    participant H2O as H2O AutoML
    
    PO->>MBA: Generar cÃ³digo de entrenamiento
    MBA->>MBA: Crear script Python + H2O
    MBA->>CEA: Ejecutar cÃ³digo
    CEA->>H2O: Entrenar modelos
    H2O->>CEA: Modelos entrenados
    CEA->>AA: Validar resultados
    AA->>AA: Evaluar mÃ©tricas
    AA->>PO: ValidaciÃ³n completada
```

### **Fase 3: Predicciones y VisualizaciÃ³n**
```mermaid
sequenceDiagram
    participant PO as Pipeline Orchestrator
    participant PA as PredictionAgent
    participant VA as VisualizationAgent
    participant CEA as CodeExecutorAgent
    
    PO->>PA: Generar predicciones
    PA->>CEA: Ejecutar script predicciÃ³n
    CEA->>PA: Predicciones generadas
    PA->>PO: Predicciones completadas
    PO->>VA: Crear visualizaciones
    VA->>CEA: Ejecutar script grÃ¡ficos
    CEA->>VA: GrÃ¡ficos generados
    VA->>PO: VisualizaciÃ³n completada
```

## ğŸ—„ï¸ Arquitectura de Datos

### **Base de Datos Principal (SQLite)**
```sql
-- Pipelines de ML
CREATE TABLE pipelines (
    id TEXT PRIMARY KEY,
    name TEXT,
    status TEXT,
    user_objective TEXT,
    file_path TEXT,
    created_at TIMESTAMP,
    updated_at TIMESTAMP
);

-- Jobs individuales por agente
CREATE TABLE jobs (
    id TEXT PRIMARY KEY,
    pipeline_id TEXT,
    agent_name TEXT,
    status TEXT,
    input_data TEXT,
    output_data TEXT,
    logs TEXT,
    created_at TIMESTAMP
);

-- Modelos entrenados
CREATE TABLE models (
    id TEXT PRIMARY KEY,
    pipeline_id TEXT,
    model_type TEXT,
    metrics TEXT,
    file_path TEXT,
    created_at TIMESTAMP
);

-- Predicciones generadas
CREATE TABLE predictions (
    id TEXT PRIMARY KEY,
    model_id TEXT,
    prediction_data TEXT,
    confidence_interval TEXT,
    created_at TIMESTAMP
);
```

### **Sistema de Archivos**
```
â”œâ”€â”€ uploads/          # Datasets cargados por usuarios
â”œâ”€â”€ models/           # Modelos entrenados guardados
â”œâ”€â”€ results/          # Predicciones y resultados
â”œâ”€â”€ visualizations/   # GrÃ¡ficos generados
â”œâ”€â”€ coding/           # Scripts generados por agentes
â”‚   â””â”€â”€ pipeline_id/  # Scripts especÃ­ficos por pipeline
â””â”€â”€ logs/            # Logs del sistema
```

## ğŸ³ Arquitectura de Contenedores

### **Contenedor Principal (Sistema)**
```dockerfile
FROM python:3.8-slim
# FastAPI + Agentes + Base de datos
EXPOSE 8006
```

### **Contenedores de EjecuciÃ³n (DinÃ¡micos)**
```dockerfile
FROM python:3.8
# InstalaciÃ³n automÃ¡tica de dependencias
# EjecuciÃ³n aislada de cÃ³digo generado
```

### **Red de Contenedores**
```mermaid
graph LR
    subgraph "Docker Network"
        MC[Main Container<br/>Port 8006]
        EC1[Exec Container 1]
        EC2[Exec Container 2]
        ECN[Exec Container N]
    end
    
    MC --> EC1
    MC --> EC2
    MC --> ECN
    
    EC1 --> H2O[H2O AutoML]
    EC2 --> H2O
    ECN --> H2O
```

## ğŸ”Œ Integraciones Externas

### **Modelos de Lenguaje**
```python
# ConfiguraciÃ³n dual: Local + Cloud
LLM_CONFIG = {
    "primary": "ollama",      # Local con gpt-oss:120b
    "fallback": "huggingface", # Cloud API
    "model": "gpt-oss:120b"
}
```

### **H2O AutoML**
```python
# IntegraciÃ³n automÃ¡tica
h2o.init()
aml = H2OAutoML(
    max_models=20,
    seed=42,
    max_runtime_secs=1800,
    sort_metric="RMSE"
)
```

## ğŸ“Š Monitoreo y Observabilidad

### **MÃ©tricas del Sistema**
- Tiempo de respuesta por agente
- Uso de recursos (CPU, memoria)
- Tasa de Ã©xito de pipelines
- Latencia de predicciones

### **Logs Estructurados**
```json
{
  "timestamp": "2024-01-01T10:00:00Z",
  "level": "INFO",
  "agent": "DataProcessorAgent",
  "pipeline_id": "pipeline_123",
  "message": "Dataset analysis completed",
  "metrics": {
    "rows": 10000,
    "columns": 15,
    "processing_time": 45.2
  }
}
```

## ğŸ”’ Consideraciones de Seguridad

### **Aislamiento de EjecuciÃ³n**
- Contenedores Docker con recursos limitados
- Red aislada para ejecuciÃ³n de cÃ³digo
- Timeout automÃ¡tico para prevenir loops infinitos

### **ValidaciÃ³n de Entrada**
- SanitizaciÃ³n de nombres de archivos
- ValidaciÃ³n de formato de datasets
- LÃ­mites de tamaÃ±o de archivos

### **AuditorÃ­a**
- Log de todas las operaciones
- Trazabilidad completa de pipelines
- RetenciÃ³n configurable de logs

## ğŸš€ Escalabilidad y Rendimiento

### **Escalabilidad Horizontal**
- MÃºltiples instancias del servicio principal
- Load balancer para distribuciÃ³n de carga
- Base de datos con conexiÃ³n pooling

### **Optimizaciones**
- Cache de resultados frecuentes
- Procesamiento asÃ­ncrono de jobs
- CompresiÃ³n de datos grandes

---

Esta arquitectura proporciona una base sÃ³lida para un sistema de Machine Learning automatizado que es **escalable**, **seguro** y **mantenible**.

**Siguiente**: [InstalaciÃ³n y ConfiguraciÃ³n](03_installation.md)
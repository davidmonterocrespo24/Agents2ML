# DataProcessorAgent

## üéØ Descripci√≥n General

El **DataProcessorAgent** es el primer agente especializado en el pipeline de Machine Learning y act√∫a como el **experto en an√°lisis de datos**. Su responsabilidad principal es analizar, validar y procesar datasets cargados por usuarios, proporcionando informaci√≥n detallada sobre la estructura, calidad y caracter√≠sticas de los datos.

## üîß Funcionalidades Principales

### üìä **An√°lisis Autom√°tico de Datasets**
- Detecci√≥n autom√°tica de separadores de columnas (`,`, `;`, `\t`, etc.)
- Identificaci√≥n de separadores decimales (`.` vs `,`)
- Detecci√≥n autom√°tica de encoding (UTF-8, Latin-1, etc.)
- Validaci√≥n de integridad del archivo CSV

### üîç **Inspecci√≥n de Estructura**
- An√°lisis de tipos de datos por columna
- Identificaci√≥n de columnas num√©ricas, categ√≥ricas y temporales
- Detecci√≥n de valores nulos y su distribuci√≥n
- Identificaci√≥n de outliers y valores an√≥malos

### üéØ **Identificaci√≥n de Objetivos**
- Sugerencia autom√°tica de columnas objetivo (target)
- Clasificaci√≥n del tipo de problema ML (regresi√≥n, clasificaci√≥n, series temporales)
- Identificaci√≥n de variables predictoras relevantes

### üìà **An√°lisis Estad√≠stico**
- Estad√≠sticas descriptivas completas
- Distribuciones de frecuencia
- Correlaciones entre variables
- An√°lisis de tendencias temporales (si aplica)

## üõ†Ô∏è Herramientas Disponibles

El DataProcessorAgent utiliza dos herramientas especializadas en un proceso de dos pasos:

### **1. get_sample_func**
**Prop√≥sito**: Inspecci√≥n inicial del archivo
- Obtiene una muestra peque√±a del archivo (primeras l√≠neas)
- Permite identificar formato antes de procesamiento completo
- Evita errores de lectura por par√°metros incorrectos

**Uso**:
```python
# El agente llama internamente:
sample = get_sample_func(file_path)
# Retorna: Primeras 5-10 l√≠neas del archivo como texto
```

### **2. read_and_analyze_func**
**Prop√≥sito**: An√°lisis completo del dataset
- Lee el archivo completo con par√°metros detectados
- Realiza an√°lisis estad√≠stico profundo
- Genera reporte completo de calidad de datos

**Par√°metros**:
- `separator`: Separador de columnas detectado
- `decimal`: Separador decimal identificado
- Otros par√°metros de pandas seg√∫n necesidad

## üìã Proceso de An√°lisis

### **Paso 1: Inspecci√≥n Inicial**
```mermaid
flowchart TD
    A[Recibir archivo CSV] --> B[Obtener muestra]
    B --> C[Detectar separadores]
    C --> D[Identificar encoding]
    D --> E[Validar formato]
    E --> F[Preparar par√°metros]
```

### **Paso 2: An√°lisis Completo**
```mermaid
flowchart TD
    A[Leer archivo completo] --> B[An√°lisis de tipos]
    B --> C[Estad√≠sticas descriptivas]
    C --> D[Detecci√≥n de outliers]
    D --> E[An√°lisis de correlaciones]
    E --> F[Identificar target]
    F --> G[Generar reporte]
```

## üìä Ejemplo de An√°lisis

### **Input: Dataset de Ventas**
```csv
fecha,ventas,mes,dia_semana,promocion
2023-01-01,1234.56,1,6,0
2023-01-02,1345.78,1,0,1
2023-01-03,1156.90,1,1,0
...
```

### **Output: Reporte de An√°lisis**
```json
{
  "file_info": {
    "filename": "ventas.csv",
    "rows": 365,
    "columns": 5,
    "size_mb": 0.05,
    "encoding": "utf-8",
    "separator": ",",
    "decimal": "."
  },
  "column_analysis": {
    "fecha": {
      "type": "datetime",
      "null_count": 0,
      "unique_count": 365,
      "format": "YYYY-MM-DD"
    },
    "ventas": {
      "type": "numeric",
      "null_count": 0,
      "min": 856.23,
      "max": 1567.89,
      "mean": 1234.45,
      "std": 123.67,
      "outliers": 3
    },
    "mes": {
      "type": "categorical",
      "null_count": 0,
      "unique_values": [1,2,3,4,5,6,7,8,9,10,11,12],
      "mode": 6
    },
    "dia_semana": {
      "type": "categorical", 
      "null_count": 0,
      "unique_values": [0,1,2,3,4,5,6],
      "distribution": "uniform"
    },
    "promocion": {
      "type": "binary",
      "null_count": 0,
      "true_ratio": 0.2,
      "false_ratio": 0.8
    }
  },
  "ml_recommendations": {
    "problem_type": "time_series_regression",
    "target_column": "ventas",
    "predictor_columns": ["mes", "dia_semana", "promocion"],
    "temporal_column": "fecha",
    "suggested_algorithms": ["AutoML", "GBM", "RandomForest"]
  },
  "data_quality": {
    "completeness": 1.0,
    "consistency": 0.98,
    "accuracy": 0.95,
    "issues": [
      "3 outliers in 'ventas' column",
      "Consider feature engineering for temporal patterns"
    ],
    "recommendations": [
      "Data is ready for ML training",
      "Consider adding lag features for time series",
      "Outliers may need investigation"
    ]
  }
}
```

## üéØ Configuraci√≥n del Agente

### **Prompt del Sistema**
El DataProcessorAgent utiliza un prompt especializado que le instruye sobre:
- Proceso de an√°lisis en dos pasos
- Mejores pr√°cticas de an√°lisis de datos
- Identificaci√≥n de problemas comunes
- Generaci√≥n de reportes estructurados

### **Modelo de Lenguaje**
- **Modelo**: gpt-oss:120b (via Ollama o Hugging Face)
- **Temperatura**: 0.1 (respuestas determin√≠sticas)
- **Max Tokens**: 4000 (para an√°lisis detallados)

## üîÑ Integraci√≥n con Otros Agentes

### **Flujo de Comunicaci√≥n**
```mermaid
sequenceDiagram
    participant U as UserProxyAgent
    participant DP as DataProcessorAgent
    participant MB as ModelBuilderAgent
    
    U->>DP: Analizar dataset
    DP->>DP: Inspeccionar muestra
    DP->>DP: An√°lisis completo
    DP->>U: Reporte de an√°lisis
    U->>MB: Enviar an√°lisis para construcci√≥n
```

### **Datos Compartidos**
- **A ModelBuilderAgent**: Tipo de problema, columnas objetivo, tipos de datos
- **A UserProxyAgent**: Estado del an√°lisis, problemas encontrados
- **A AnalystAgent**: M√©tricas de calidad de datos

## üêõ Manejo de Errores

### **Errores Comunes y Soluciones**

#### **Error: Archivo no encontrado**
```python
# Error handling interno del agente
if not os.path.exists(file_path):
    return {
        "error": "File not found",
        "message": "Please verify the file path",
        "suggestions": ["Check file exists", "Verify permissions"]
    }
```

#### **Error: Formato no reconocido**
```python
# Intentos m√∫ltiples de lectura
separators = [',', ';', '\t', '|']
for sep in separators:
    try:
        df = pd.read_csv(file_path, sep=sep, nrows=5)
        if len(df.columns) > 1:
            detected_separator = sep
            break
    except:
        continue
```

#### **Error: Encoding incorrecto**
```python
# Detecci√≥n autom√°tica de encoding
encodings = ['utf-8', 'latin-1', 'cp1252', 'iso-8859-1']
for encoding in encodings:
    try:
        with open(file_path, 'r', encoding=encoding) as f:
            sample = f.read(1000)
        detected_encoding = encoding
        break
    except UnicodeDecodeError:
        continue
```

## üìä M√©tricas y Monitoreo

### **M√©tricas de Rendimiento**
- **Tiempo de an√°lisis**: T√≠picamente 30-60 segundos
- **Precisi√≥n de detecci√≥n**: >95% para formatos est√°ndar
- **Cobertura de tipos**: Num√©rico, categ√≥rico, temporal, texto

### **Logs de Actividad**
```json
{
  "timestamp": "2024-01-01T10:30:00Z",
  "agent": "DataProcessorAgent",
  "pipeline_id": "pipeline_123",
  "action": "analyze_dataset",
  "file_info": {
    "name": "sales.csv",
    "size": "1.2MB",
    "rows": 10000
  },
  "execution_time": 45.2,
  "status": "completed",
  "issues_found": 2
}
```

## üîß Personalizaci√≥n y Extensi√≥n

### **A√±adir Nuevos Tipos de Datos**
```python
# Extender detecci√≥n de tipos
def detect_custom_types(column):
    if column.name.endswith('_id'):
        return 'identifier'
    elif 'email' in column.name.lower():
        return 'email'
    elif 'phone' in column.name.lower():
        return 'phone'
    # ... m√°s detecciones personalizadas
```

### **Configurar Umbrales**
```python
# Configuraci√≥n personalizable
ANALYSIS_CONFIG = {
    "outlier_threshold": 3.0,  # Desviaciones est√°ndar
    "null_warning_threshold": 0.05,  # 5% valores nulos
    "correlation_threshold": 0.7,  # Correlaci√≥n alta
    "sample_size": 1000  # Filas para an√°lisis inicial
}
```

## üìö Best Practices

### **Para Desarrolladores**
1. **Validar entrada**: Siempre verificar formato antes de an√°lisis completo
2. **Manejo robusto**: Implementar fallbacks para formatos no est√°ndar
3. **Performance**: Usar muestreo para datasets grandes
4. **Logging**: Registrar todos los pasos para debugging

### **Para Usuarios**
1. **Formato consistente**: Usar separadores est√°ndar (coma)
2. **Encoding**: Preferir UTF-8 cuando sea posible
3. **Headers**: Incluir nombres de columna descriptivos
4. **Calidad**: Minimizar valores nulos y inconsistencias

## üîç Troubleshooting

### **Problema: An√°lisis muy lento**
```python
# Soluci√≥n: Configurar muestreo
SAMPLE_CONFIG = {
    "max_rows": 10000,  # Limitar filas para an√°lisis
    "sample_ratio": 0.1  # Usar 10% del dataset si es muy grande
}
```

### **Problema: Detecci√≥n incorrecta de tipos**
```python
# Soluci√≥n: Configuraci√≥n manual de tipos
TYPE_HINTS = {
    'id': 'string',
    'fecha': 'datetime',
    'categoria': 'categorical'
}
```

---

El **DataProcessorAgent** es fundamental para el √©xito del pipeline, ya que la calidad del an√°lisis inicial determina la efectividad de todos los pasos posteriores.

**Siguiente**: [ModelBuilderAgent](model_builder_agent.md)